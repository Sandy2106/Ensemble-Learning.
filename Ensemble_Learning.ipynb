{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "2SvPrB1EisWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?\n",
        "- Yes, Bagging can be used for regression problems using models like Bagging Regressor. It combines predictions from multiple regressors by averaging their outputs to reduce variance and improve performance.\n",
        "\n",
        "\n",
        "2. What is the difference between multiple model training and single model training?\n",
        "- Single model training: Trains one model on the entire dataset.\n",
        "\n",
        "- Multiple model training: Trains several models on different subsets (or variations) of the dataset and combines their outputs (ensemble), leading to better generalization and robustness.\n",
        "\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "- In Random Forest, at each split in a decision tree, only a random subset of features is considered. This promotes diversity among trees and reduces correlation, leading to a more generalized ensemble.\n",
        "\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "- OOB score is an internal validation method used in Bagging/Random Forest. For each tree, data not included in its bootstrap sample (OOB data) is used to evaluate performance. It's a form of cross-validation.\n",
        "\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "- Feature importance in Random Forest can be measured by:\n",
        "\n",
        "  - Gini Importance: Total decrease in impurity contributed by a feature.\n",
        "\n",
        "  - Permutation Importance: Decrease in model performance when a feature’s values are randomly shuffled.\n",
        "6. Explain the working principle of a Bagging Classifier.\n",
        "- Bootstrap sampling: Random subsets with replacement.\n",
        "- Train multiple base classifiers (e.g., Decision Trees) on each subset.\n",
        "- Aggregate predictions by majority vote (for classification).\n",
        "\n",
        "7. How do you evaluate a Bagging Classifier’s performance?\n",
        "- Use metrics like:\n",
        "  - Accuracy, Precision, Recall, F1-Score (for classification)\n",
        "  - OOB Score as a quick internal validation\n",
        "  - Cross-validation for robustness\n",
        "8. How does a Bagging Regressor work?\n",
        "- Trains multiple regressors on different bootstrap samples.\n",
        "- Combines predictions by averaging.\n",
        "- Reduces variance without increasing bias\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "- They combine multiple models to improve performance, reduce overfitting, and enhance generalization compared to individual models.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "- Complexity: Harder to interpret and maintain.\n",
        "- Computational cost: Requires more memory and time to train multiple models.\n",
        "\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques.\n",
        "- The core idea is that a group of weak learners can come together to form a strong learner by combining their predictions, thereby reducing bias and/or variance.\n",
        "\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "- An ensemble model that builds multiple decision trees using bootstrap samples and random feature subsets, then predicts by majority voting.\n",
        "\n",
        "13. What are the main types of ensemble techniques?\n",
        "- Bagging (Bootstrap Aggregating)\n",
        "- Boosting (e.g., AdaBoost, Gradient Boosting)\n",
        "- Stacking (model of models)\n",
        "\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "- A technique where multiple models (classifiers or regressors) are trained and their predictions combined to improve overall performance.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "- When interpretability is critical.\n",
        "- When the dataset is small and simple models perform well.\n",
        "- When training time/resources are limited.\n",
        "\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "- Bagging reduces overfitting by:\n",
        "\n",
        "   - Training on different subsets of data (bootstrapping),\n",
        "\n",
        "   - Averaging predictions, which reduces variance.\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "- Random Forest reduces overfitting and improves generalization by:\n",
        "  - Using multiple trees trained on bootstrapped data,\n",
        "  - Introducing randomness in feature selection.\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "- Bootstrap sampling provides different training subsets for each base model, encouraging model diversity and reducing variance when aggregating.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "- Fraud detection\n",
        "- Medical diagnosis\n",
        "- Credit scoring\n",
        "- Spam filtering\n",
        "- Recommendation systems\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "- Bagging:-\n",
        " Reduce variance,\n",
        " Parallel,\n",
        " Uniform.\n",
        "- Boosting:-\n",
        "  Reduce bias\n",
        "  Sequential (each improves prior)\n",
        "  Adjusted after each round\n"
      ],
      "metadata": {
        "id": "eJoY89JuixOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "oYqvopdammsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "467oEhHxmpBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "IWW9oRj7td3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "DIqegz56og9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Tree as base estimator\n",
        "bagging_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "3Ee1A8E5tetj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "4LwWV5C0onsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance Score': importances\n",
        "}).sort_values(by='Importance Score', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"Feature Importance Scores (Descending Order):\\n\")\n",
        "print(feature_importance_df.to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "id": "qyWG2ft2tfLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "eDi0-XMxonfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "dt_preds = dt_regressor.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_preds)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_preds = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "# Print the comparison results\n",
        "print(f\"Decision Tree Regressor MSE: {dt_mse:.2f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "gzj1s5Qktf22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "fH89WGvyonRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets (although OOB score uses only training data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest with OOB enabled\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,         # Enable Out-of-Bag scoring\n",
        "    bootstrap=True,         # Required for OOB to work\n",
        "    random_state=42\n",
        ")\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB Score\n",
        "print(f\"Out-of-Bag (OOB) Score: {rf_clf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "arNjxjJ1tgWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "jLw2RCDHonDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier using SVM as base estimator\n",
        "bagging_svm = BaggingClassifier(\n",
        "    base_estimator=SVC(),\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "grh0v5PCthkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "HyGmpv8oom1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Try different numbers of trees\n",
        "tree_counts = [1, 5, 10, 50, 100, 200]\n",
        "print(\"Random Forest Accuracy with Different Numbers of Trees:\\n\")\n",
        "print(f\"{'Trees':<10} {'Accuracy'}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Loop through different tree counts and evaluate accuracy\n",
        "for n in tree_counts:\n",
        "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{n:<10} {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "VGiPmqQPth9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "DA0YsRn5oml5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary labels: 0 or 1\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with Logistic Regression as base estimator\n",
        "bagging_lr = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(max_iter=1000, solver='liblinear'),  # Ensure convergence\n",
        "    n_estimators=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = bagging_lr.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute AUC score\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "HC00rfjXtifg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "5L2kWO6FomZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_regressor.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display feature importance scores\n",
        "print(\"Random Forest Regressor - Feature Importance Scores:\\n\")\n",
        "print(feature_importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "m5P8ByE9tjKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "e_JmyfXUomMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# 2. Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_preds = rf_model.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_preds)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Model Accuracy Comparison:\\n\")\n",
        "print(f\"Bagging Classifier Accuracy      : {bagging_acc:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ilGUlaHLtjvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV."
      ],
      "metadata": {
        "id": "fTYjXWyvol8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,   # Use all available cores for parallel processing\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and corresponding cross-validation accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy: {:.4f}\".format(grid_search.best_score_))\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Set Accuracy: {:.4f}\".format(test_accuracy))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "6M6YIFBktkYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ],
      "metadata": {
        "id": "rOGOoLrbolmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of different n_estimators to evaluate\n",
        "n_estimators_list = [1, 5, 10, 50, 100, 200]\n",
        "\n",
        "print(\"Bagging Regressor Performance with Varying Base Estimators:\\n\")\n",
        "print(f\"{'Estimators':<12}{'MSE':>10}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "# Loop through different values of n_estimators\n",
        "for n in n_estimators_list:\n",
        "    # Train Bagging Regressor\n",
        "    bagging = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate MSE\n",
        "    y_pred = bagging.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Print result\n",
        "    print(f\"{n:<12}{mse:>10.4f}\")\n"
      ],
      "metadata": {
        "id": "Vt3IZWj2tk7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "IR8NdgqRolUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_indices = (y_test != y_pred)\n",
        "\n",
        "# Create a DataFrame with test data and predictions\n",
        "result_df = X_test.copy()\n",
        "result_df['Actual'] = y_test\n",
        "result_df['Predicted'] = y_pred\n",
        "result_df['Correct'] = y_test == y_pred\n",
        "\n",
        "# Filter misclassified samples\n",
        "misclassified_samples = result_df[~result_df['Correct']]\n",
        "\n",
        "# Display misclassified samples\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_samples[['Actual', 'Predicted'] + list(X.columns)].head())\n"
      ],
      "metadata": {
        "id": "4aermC3otl-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "2V0RSVYiolF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train a single Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_preds = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# 2. Train a Bagging Classifier with Decision Trees\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_preds = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_preds)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(\"Performance Comparison:\")\n",
        "print(f\"Single Decision Tree Accuracy : {dt_accuracy:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy   : {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZyyOq1dttmlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.  Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "hr2V8x9Qok3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BnRcZdbhtnGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy."
      ],
      "metadata": {
        "id": "djbtiauwokdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "base_estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42))  # SVC must have probability=True for stacking\n",
        "]\n",
        "\n",
        "# Define meta-learner (final estimator)\n",
        "meta_learner = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create the Stacking Classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=meta_learner,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Train stacking model\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "stacking_preds = stacking_model.predict(X_test)\n",
        "stacking_acc = accuracy_score(y_test, stacking_preds)\n",
        "\n",
        "# Train and evaluate individual models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "dt.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy for individual models\n",
        "acc_dt = accuracy_score(y_test, dt.predict(X_test))\n",
        "acc_svm = accuracy_score(y_test, svm.predict(X_test))\n",
        "acc_lr = accuracy_score(y_test, lr.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy Comparison:\")\n",
        "print(f\"Decision Tree Accuracy       : {acc_dt:.4f}\")\n",
        "print(f\"SVM Accuracy                 : {acc_svm:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "BY4QOiq6tn-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "QmYj048aokM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "features = X.columns\n",
        "\n",
        "# Create DataFrame for feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance and display top 5\n",
        "top_5_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "id": "JyfuEUlktoZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "3zAxdNdQoj_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with Decision Tree base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate using precision, recall, and F1-score\n",
        "print(\"Classification Report (Precision, Recall, F1-score):\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "b0m82zIyto4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "C6uQ3fgCojzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Evaluate effect of max_depth\n",
        "depths = list(range(1, 21))\n",
        "accuracies = []\n",
        "\n",
        "for depth in depths:\n",
        "    rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(depths, accuracies, marker='o', linestyle='-', color='green')\n",
        "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.xticks(depths)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tDQZ0SBmtpd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance."
      ],
      "metadata": {
        "id": "31v451Quojod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging with Decision Tree\n",
        "bagging_dt = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "dt_preds = bagging_dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_preds)\n",
        "\n",
        "# Bagging with K-Nearest Neighbors\n",
        "bagging_knn = BaggingRegressor(\n",
        "    base_estimator=KNeighborsRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "knn_preds = bagging_knn.predict(X_test)\n",
        "knn_mse = mean_squared_error(y_test, knn_preds)\n",
        "\n",
        "# Print comparison\n",
        "print(\"Bagging Regressor Performance Comparison:\")\n",
        "print(f\"Decision Tree Base Estimator MSE: {dt_mse:.4f}\")\n",
        "print(f\"K-Neighbors Base Estimator MSE   : {knn_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "dn-h3PF1tqfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "Rl8LyMjnojdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"Random Forest ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "MupB4SI9trKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio.\n"
      ],
      "metadata": {
        "id": "B169YWr0ojQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define Bagging Classifier with Decision Tree base estimator\n",
        "bagging_model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Evaluate with 5-fold cross-validation\n",
        "cv_scores = cross_val_score(bagging_model, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Classifier Cross-Validation Results:\")\n",
        "print(f\"Fold Accuracies : {cv_scores}\")\n",
        "print(f\"Mean Accuracy   : {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard Deviation : {np.std(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "90R9c358tr_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve\n"
      ],
      "metadata": {
        "id": "m0d670ooojEa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ijtc6Yz-ttSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "igOMB9ydoi21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base models\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Define stacking classifier\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=[('rf', rf), ('lr', lr)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "# Fit individual models\n",
        "rf.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "stack_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "stack_acc = accuracy_score(y_test, stack_model.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"Random Forest Accuracy      : {rf_acc:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_acc:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stack_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "3S4B2ESvttpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "o_II2ljMoitH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different max_samples values (proportions of training data)\n",
        "sample_sizes = [0.3, 0.5, 0.7, 1.0]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate for each bootstrap sample size\n",
        "for size in sample_sizes:\n",
        "    model = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=100,\n",
        "        max_samples=size,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"max_samples = {size} --> MSE: {mse:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(sample_sizes, mse_scores, marker='o', linestyle='--', color='blue')\n",
        "plt.title('Effect of max_samples on Bagging Regressor Performance')\n",
        "plt.xlabel('max_samples (proportion of training data)')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KcLhiQ3StuUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}